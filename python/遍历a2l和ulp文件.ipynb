{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批量parsing a2l/ulp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "# 遍历文件夹\n",
    "def walkFile(file):\n",
    "    for root, dirs, files in os.walk(file):\n",
    "\n",
    "        # root 表示当前正在访问的文件夹路径\n",
    "        # dirs 表示该文件夹下的子目录名list\n",
    "        # files 表示该文件夹下的文件list\n",
    "        for d in dirs:\n",
    "            directory_files_in = os.path.join(root, d)\n",
    "            print(os.listdir(directory_files_in))\n",
    "        \n",
    "        # 遍历文件\n",
    "        for f in files:\n",
    "            print(\"### \" + os.path.join(root, f))\n",
    "            \n",
    "def list_all_files(directory):\n",
    "    files_list = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for f in files:\n",
    "            file = os.path.join(root, f)\n",
    "            if file.endswith(\"a2l\") or file.endswith(\"ulp\") or file.endswith(\"dat\") or file.endswith(\"csv\") or file.endswith(\"json\"):\n",
    "                if \"(\" in file or \")\" in file:\n",
    "                    file_ = file.replace(\"(\", \"\")\n",
    "                    file_ = file_.replace(\")\", \"_\")\n",
    "                    os.rename(file, file_)\n",
    "                files_list.append(file)\n",
    "    return files_list\n",
    "\n",
    "\n",
    "def check_file_prefix(prefix, file):\n",
    "    if file.starswith(prefix):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def group_file_with_same_exp(file_list):\n",
    "    result = {}\n",
    "    for f in file_list:\n",
    "        # get file prefix\n",
    "        f_parts = f.split(\"/\")\n",
    "        f_prefix = \"/\".join(f_parts[0:-1])\n",
    "        # add prefix key to result dict\n",
    "        if f_prefix not in result:\n",
    "            result[f_prefix] = []\n",
    "        result[f_prefix].append(f)\n",
    "    \n",
    "    return result\n",
    "        \n",
    "def output_to_file(dict_, directory):\n",
    "    for key in dict_:\n",
    "        json_output = os.path.join(key, \"parse_calibration.json\")\n",
    "        parts = key.split(\"/\")\n",
    "        file_name = parts[-1]\n",
    "        path = os.path.join(directory, file_name)\n",
    "        f = open(path, \"w\")\n",
    "        files = dict_[key]\n",
    "        l = [\"\", \"\"]\n",
    "        for it in files:\n",
    "            if it.endswith(\"a2l\"):\n",
    "                l[0] = it\n",
    "            if it.endswith(\"ulp\"):\n",
    "                l[1] = it\n",
    "        f.write(l[0])\n",
    "        f.write(\"\\n\")\n",
    "        f.write(l[1])\n",
    "        f.write(\"\\n\")\n",
    "        f.write(json_output)\n",
    "        with open(json_output, \"w\") as f_out:\n",
    "            pass\n",
    "        f.close()\n",
    "        \n",
    "    \n",
    "        print(\"echo    Parsing ...  \")\n",
    "        java_cmd = \"java -jar znbd-1.0-SNAPSHOT.jar  com.psagroup.calibrationparserapi.CalibrationParserApi2 %s %s %s > stderr 2>&1 \"%(\n",
    "            l[0], l[1], json_output\n",
    "        )\n",
    "        print(java_cmd)\n",
    "        print(\"echo    Parsing OK \")\n",
    "        \n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_dirs = '/Users/others2/Desktop/calibration/data/'\n",
    "\n",
    "# a = list_all_files(file_dirs)\n",
    "# b = group_file_with_same_exp(a)\n",
    "# output_to_file(b, \"/Users/others2/Desktop/output_ulp_json/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重新组织map文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ACM_EGR_DMND_A0_NORM_APM', 'ITD_AFTER_SEP_SMOKE_NORM_APM', 'P_T_SCR_NH3_STORED_TARGET_APM', 'P_T_SCR_UREA_FLOW_LIMIT_APM', 'P_T_LNT_NOXR_ABORT_IMEP_MAX_APM', 'T_D_ENG_MAX_BRAKE_TORQUE_APM', 'AFC_SMOKE_LIM_A0_NORM_APM', 'FQD_AFTER_SMOKE_CONT_NORM_APM', 'T_D_MAX_FUEL_NORM_APM', 'T_D_PEDAL_POS_TO_TORQUE_0_APM', 'ITD_MAIN_TMNG_DMND_NORM_APM', 'PSE_INTERCOOLER_PRESS_DROP_APM', 'P_T_SCR_NH3_TARGET_LIMIT_APM', 'P_T_LNT_NOXR_RETRG_IMEP_MIN_APM', 'P_T_LNT_NOXR_TRG_IMEP_MIN_APM', 'P_T_SCR_EST_NH3_CORRECTION_APM', 'T_D_MAX_TORQ_COOL_TEMP_SCALE_APM', 'RPD_RAILP_DEMAND_NORM_APM', 'P_T_LNT_NOXR_TRG_DIMEP_APM', 'ACM_EGR_DMND_INERT_MIN_NORM_APM', 'P_T_LNT_NOXR_ABORT_IMEP_MIN_APM', 'ACM_VGT_BST_LIM_E_A0_NORM_APM', 'ITD_PILOT1_SEP_DMND_NORM_APM', 'P_T_SCR_NOX_NH3_RATIO_APM'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import interpolate\n",
    "\n",
    "# 先采集几个重要的标定量和采集量\n",
    "map_file = \"/Users/others2/Desktop/MAP.txt\"\n",
    "important_map = set()\n",
    "sample_file = \"/Users/others2/Desktop/sample.txt\"\n",
    "important_sample = set()\n",
    "with open(map_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        important_map.add(line)\n",
    "with open(sample_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        important_sample.add(line)\n",
    "#important_map = {\"AFC_SMOKE_LIM_A0_NORM_APM\"}\n",
    "print(important_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map stored in A2lMap class\n",
    "class A2lMap():\n",
    "    def __init__(self, name, desc, data, char_type):\n",
    "        self.name = name\n",
    "        self.desc = desc\n",
    "        self.data = data\n",
    "        self.char_type = char_type\n",
    "        self.axisX = None\n",
    "        self.axisY = None\n",
    "        self.value = None\n",
    "        self.get()\n",
    "    \n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        解析MAP\n",
    "        \"\"\"\n",
    "        self.axisX = [float(v) for v in self.data[\"axisX\"][\"value\"]]\n",
    "        self.inputqX = self.data[\"axisX\"][\"inputquantity\"]\n",
    "        self.unitX = self.data[\"axisX\"][\"unit\"]\n",
    "        \n",
    "        self.axisY = [float(v) for v in self.data[\"axisY\"][\"value\"]]\n",
    "        self.inputqY = self.data[\"axisY\"][\"inputquantity\"]\n",
    "        self.unitY = self.data[\"axisY\"][\"unit\"]\n",
    "        \n",
    "        x_size = len(self.axisX)\n",
    "        y_size = len(self.axisY)\n",
    "            \n",
    "        readway = self.data[\"data\"][\"readway\"]\n",
    "        rows = [None for i in range(y_size)]\n",
    "        if readway == \"ROW_DIR\":\n",
    "            for idx in range(y_size):\n",
    "                row = self.data[\"data\"][\"value\"][idx*x_size : (idx+1)*x_size]\n",
    "                rows[idx] = row  \n",
    "            rows = np.array(rows).T\n",
    "        self.value = rows\n",
    "        \n",
    "        row_number = self.value.shape[0]\n",
    "        col_number = self.value.shape[1]\n",
    "        \n",
    "        assert isinstance(self.axisX, list), \"axisX must be a list\"\n",
    "        assert isinstance(self.axisY, list), \"axisY must be a list\"\n",
    "        assert len(self.axisX) == row_number, \"axisX length not match\"\n",
    "        assert len(self.axisY) == col_number, \"axisY length not match\"\n",
    "            \n",
    "        \n",
    "    def lookup(self, x, y):\n",
    "        \"\"\"\n",
    "        根据标定量MAP的两个坐标轴的输入，通过双线性插值的方法获取对应的值\n",
    "        \"\"\"\n",
    "        x_sz = len(self.axisX)\n",
    "        y_sz = len(self.axisY)\n",
    "        x_idx = 0\n",
    "        y_idx = 0\n",
    "        x_idx_before = x_idx\n",
    "        y_idx_before = y_idx\n",
    "        \n",
    "        #print(\"x: %f, y: %f\" % (x, y))\n",
    "        # search axisX\n",
    "        if x <= self.axisX[0]:\n",
    "            x_idx = 0\n",
    "        elif x>= self.axisX[x_sz-1]:\n",
    "            x_idx = x_sz - 1\n",
    "        else:\n",
    "            while x >= self.axisX[x_idx] and x_idx < x_sz:\n",
    "                x_idx += 1\n",
    "        \n",
    "        # search axisY\n",
    "        if y <= self.axisY[0]:\n",
    "            y_idx = 0\n",
    "        elif y>= self.axisY[y_sz-1]:\n",
    "            y_idx = y_sz - 1\n",
    "        else:\n",
    "            while y >= self.axisY[y_idx] and y_idx < y_sz:\n",
    "                y_idx += 1\n",
    "        \n",
    "        #print(\"x_idx: %d, y_idx: %d\" % (x_idx, y_idx))\n",
    "        if x_idx == 0 or x_idx == x_sz - 1:\n",
    "            x_idx_before = x_idx\n",
    "        else:\n",
    "            x_idx_before = x_idx - 1\n",
    "        \n",
    "        if y_idx == 0 or y_idx == y_sz - 1:\n",
    "            y_idx_before = y_idx\n",
    "        else:\n",
    "            y_idx_before = y_idx - 1\n",
    "        \n",
    "        #print(\"x_idx_before: %d, y_idx_before: %d\" % (x_idx_before, y_idx_before))\n",
    "        \n",
    "        Q11 = self.value[x_idx][y_idx_before]\n",
    "        Q21 = self.value[x_idx][y_idx]\n",
    "        Q12 = self.value[x_idx_before][y_idx_before]\n",
    "        Q22 = self.value[x_idx_before][y_idx]\n",
    "        \n",
    "        #print(\"Q11: %f, Q21: %f, Q12: %f, Q22: %f\"%(Q11, Q21, Q12, Q22))\n",
    "\n",
    "        y_gap = self.axisY[y_idx] - self.axisY[y_idx_before]\n",
    "        x_gap = self.axisX[x_idx] - self.axisX[x_idx_before]\n",
    "        \n",
    "        if x_gap == 0 and y_gap == 0:\n",
    "            return Q11\n",
    "        \n",
    "        if x_gap == 0 and y_gap != 0:\n",
    "            percent = (y - self.axisY[y_idx_before]) / y_gap \n",
    "            return (Q21 - Q11) * percent + Q11\n",
    "        \n",
    "        if y_gap == 0 and x_gap != 0:\n",
    "            percent = (x - self.axisX[x_idx_before]) / x_gap\n",
    "            return  (Q11 - Q12) * percent + Q12\n",
    "        \n",
    "        # Y 方向插值\n",
    "        f1 = (y - self.axisY[y_idx_before]) / y_gap * (Q21 - Q11) + Q11\n",
    "        f2 = (y - self.axisY[y_idx_before]) / y_gap * (Q22 - Q12) + Q12\n",
    "        \n",
    "        # X 方向插值\n",
    "        fp = (x - self.axisX[x_idx_before]) / x_gap * (f1 - f2) + f2\n",
    "        \n",
    "        return fp\n",
    "        \n",
    "#         # Y 方向插值\n",
    "#         f1 = (self.axisY[y_idx] - y) / y_gap * Q11 + (y - self.axisY[y_idx_before]) / y_gap * Q21\n",
    "#         f2 = (self.axisY[y_idx] - y) / y_gap * Q12 + (y - self.axisY[y_idx_before]) / y_gap * Q22\n",
    "        \n",
    "#         #print(\"f1: %f, f2: %f\"%(f1, f2))\n",
    "        \n",
    "#         # X 方向插值\n",
    "#         fp = (self.axisX[x_idx] - x) / x_gap * f1 + (x - self.axisX[x_idx_before]) / x_gap * f2\n",
    "        \n",
    "#         return fp\n",
    "            \n",
    "    \n",
    "# curve stored in A2lCurve class\n",
    "class A2lCurve():\n",
    "    def __init__(self, name, desc, data, char_type):\n",
    "        \"\"\"\n",
    "        name: 标定量的名字\n",
    "        desc: 标定量的描述\n",
    "        data: 标定量的值\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.desc = desc\n",
    "        self.data = data\n",
    "        self.char_type = char_type\n",
    "        self.get()\n",
    "        \n",
    "    def get(self):\n",
    "        self.axisX = self.data[\"axisX\"][\"value\"]\n",
    "        self.inputqX = self.data[\"axisX\"][\"inputquantity\"]\n",
    "        self.unitX = self.data[\"axisX\"][\"unit\"]\n",
    "        self.value = self.data[\"data\"][\"value\"]\n",
    "        \n",
    "        assert isinstance(self.value, list), \"value must be a list\"\n",
    "        assert len(self.value) == len(self.axisX), \"axisX must be same length as value\"\n",
    "        \n",
    "    def lookup(self, x):\n",
    "        if x <= self.axisX[0]:\n",
    "            return self.value[0]\n",
    "    \n",
    "        sz = len(self.axisX)\n",
    "        if x >= self.axisX[sz-1]:\n",
    "            return self.value[sz-1]\n",
    "    \n",
    "        i = 1\n",
    "        while x<=self.axisX[i] and i<sz-1:\n",
    "            i += 1\n",
    "        gap = (x - self.axisX[i-1]) / (self.axisX[i] - self.axisX[i-1])\n",
    "        \n",
    "        return self.value[i-1] + gap * (self.value[i] - self.value[i-1])\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad_label: \n",
      "P_T_SCR_NH3_TARGET_LIMIT_APM  P_T_Scr2_stored_nh3  TSE_Scr_bed_temp\n",
      "PSE_INTERCOOLER_PRESS_DROP_APM  ACM_Compressor_flow_sel \n"
     ]
    }
   ],
   "source": [
    "def roll_to_1800s(file_directory):\n",
    "    \"\"\"\n",
    "    1800s的数据展开，map curve 查值\n",
    "    Parameters:\n",
    "        file_directory: json文件和csv文件所在的目录\n",
    "    Return:\n",
    "        \n",
    "    \"\"\"\n",
    "    files = list_all_files(file_directory)\n",
    "    a2l_json_file = \"\"\n",
    "    dat_csv = \"\"\n",
    "    \n",
    "    for name in files:\n",
    "        if name.endswith(\"json\"):\n",
    "            a2l_json_file = name\n",
    "        if name.endswith(\"csv\"):\n",
    "            dat_csv = name\n",
    "        \n",
    "    assert a2l_json_file != \"\", \"a2l json is empty\"\n",
    "    assert dat_csv != \"\", \"data csv is empty\"\n",
    "    \n",
    "    # read csv to dataframe\n",
    "    dat = pd.read_csv(dat_csv, sep=\";\")\n",
    "    dat.drop([0, 1], inplace=True)   # 去掉第一行和第二行，第一行是量纲行，第二行有空字符\n",
    "    dat.reset_index(drop=True, inplace=True)\n",
    "    columns = dat.columns\n",
    "    \n",
    "    # read json to a dict\n",
    "    js_dict = {}\n",
    "    with open(a2l_json_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            js = json.loads(line)\n",
    "            # 解析字段\n",
    "            label = js[\"label\"]         # 标定量\n",
    "            if label not in important_map:\n",
    "                continue\n",
    "            char_type = js[\"charType\"]  # 标定量的类型\n",
    "            data = eval(js[\"data\"])     # 标定量的值\n",
    "            desc = js[\"description\"]    # 标定量的描述\n",
    "            if char_type == \"MAP\":\n",
    "                a2l_map = A2lMap(label, desc, data, char_type)\n",
    "                js_dict[label] = a2l_map\n",
    "                \n",
    "            elif char_type == \"CURVE\":\n",
    "                a2l_curve = A2lCurve(label, desc, data, char_type)\n",
    "                js_dict[label] = a2l_curve\n",
    "    \n",
    "    #a2l_AFC_SMOKE_LIM_A0_NORM_APM = js_dict[\"AFC_SMOKE_LIM_A0_NORM_APM\"]\n",
    "    #print(a2l_AFC_SMOKE_LIM_A0_NORM_APM.value)\n",
    "    #print(a2l_AFC_SMOKE_LIM_A0_NORM_APM.axisX)\n",
    "    #print(a2l_AFC_SMOKE_LIM_A0_NORM_APM.axisY)\n",
    "    \n",
    "    # 添加标定量到 csv\n",
    "    for key in js_dict:\n",
    "        dat[key] = 0.0\n",
    "    \n",
    "    map_labels = list(js_dict.keys())\n",
    "    bad_label = []\n",
    "    rows_cnt = dat.shape[0]\n",
    "    for calib_var in map_labels:\n",
    "        for index in range(rows_cnt):\n",
    "            #print(\"line: %d\"%(index))\n",
    "            a2l_obj = js_dict[calib_var]\n",
    "            label = a2l_obj.name\n",
    "            char_type = a2l_obj.char_type\n",
    "            \n",
    "            #if label in [\"PSE_INTERCOOLER_PRESS_DROP_APM\", \"P_T_SCR_NH3_TARGET_LIMIT_APM\"]:\n",
    "            #    continue\n",
    "            \n",
    "            try:\n",
    "                if char_type == \"CURVE\":\n",
    "                    x_inputquantity = a2l_obj.inputqX\n",
    "                    x_inputquantity_value = float(dat.at[index, x_inputquantity])\n",
    "                    lookup_value = a2l_obj.lookup(x_inputquantity_value)\n",
    "                    dat.at[index, label] = lookup_value\n",
    "                elif char_type == \"MAP\":\n",
    "                    x_inputquantity = a2l_obj.inputqX\n",
    "                    y_inputquantity = a2l_obj.inputqY\n",
    "                    x_inputquantity_value = float(dat.at[index, x_inputquantity])\n",
    "                    y_inputquantity_value = float(dat.at[index, y_inputquantity])\n",
    "                    #print(\"x_inputquantity: %s, x_inputquantity_value:%f\"%(x_inputquantity, x_inputquantity_value))\n",
    "                    #print(\"y_inputquantity: %s, y_inputquantity_value:%f\"%(y_inputquantity, y_inputquantity_value))\n",
    "                    lookup_value = a2l_obj.lookup(x_inputquantity_value, y_inputquantity_value)\n",
    "                    dat.at[index, label] = lookup_value\n",
    "            except:\n",
    "                if char_type == \"CURVE\":\n",
    "                    bad_label.append(\"%s  %s\"%(label, a2l_obj.inputqX))\n",
    "                else:\n",
    "                    bad_label.append(\"%s  %s  %s\"%(label, a2l_obj.inputqX, a2l_obj.inputqY))\n",
    "    bad_label = list(set(bad_label))\n",
    "    print(\"bad_label: \\n%s \"%(\"\\n\".join(bad_label)))\n",
    "    return dat\n",
    "\n",
    "# test\n",
    "join_dat = roll_to_1800s(\"/Users/others2/Desktop/calibration/data/20191029--1/\")\n",
    "join_dat.to_csv(\"/Users/others2/Desktop/xx.csv\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2315.75\n",
      "463.5\n",
      "28.2471484375\n"
     ]
    }
   ],
   "source": [
    "time_step = 7000\n",
    "print(join_dat.at[time_step, \"IN_Engine_cycle_speed\"] )\n",
    "print(join_dat.at[time_step, \"AFC_Smoke_limit_dsrd_imep\"])\n",
    "print(join_dat.at[time_step, \"AFC_SMOKE_LIM_A0_NORM_APM\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.359"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2000.0 - 1800) / (2000 - 1800) *(18.359 - 18.590) + 18.590"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.43"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2000.0 - 1800) / (2000 - 1800) * (19.430 - 19.699) + 19.699"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.01484125"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2315.75 - 2200) / (2400 - 2200) *(19.430 - 18.359) + 18.395"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001898057610483655"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(19.01484125 - 18.978818359375) / 18.978818359375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.020145"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(463.5 - 500) / 100 * (27.965 - 28.738) + 27.738"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.450145"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(463.5 - 500) / 100 * (27.395 - 28.168) + 28.168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.020145"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3000 - 2800) / 200 *(28.020145 - 28.450145) + 28.450145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008101436930465646"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(28.2471484375 - 28.020145) / 28.020145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
